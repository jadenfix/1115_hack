Here‚Äôs a full blueprint you can basically follow step-by-step. I‚Äôll keep it practical and hackathon-doable.

---

## 0. Goals & Constraints

You want:

* A **Temporal-orchestrated ‚ÄúResearchCompanyWorkflow‚Äù**
* Tools:

  * **Linkup API** for company/web data search ([Linkup][1])
  * **Browser Use** to actually browse key URLs and pull facts ([GitHub][2])
  * **Raindrop / SmartBuckets / SmartMemory (LiquidMetal)** as long-term memory & run history ([Raindrop Developer Hub][3])
  * **Freepik API** to generate a visual ‚Äúaccount snapshot‚Äù ([Freepik API][4])
  * **Anthropic** for all reasoning, synthesis & self-critique
* A **self-learning loop** that updates ‚Äúbrowsing + query strategy‚Äù over time.

Think: *one Temporal workflow to research a single company*, another workflow/cron to *periodically read past runs and update the strategy*.

---

## 1. High-Level Architecture

**Components**

1. **Temporal Cluster + Worker**

   * Python worker running your workflows and activities.
2. **HTTP API or CLI**

   * Simple FastAPI CLI/web app that kicks off `ResearchCompanyWorkflow(company_name)` and then shows results.
3. **External APIs**

   * Linkup HTTP client
   * Browser Use client
   * Raindrop/SmartBuckets client
   * Freepik HTTP client
   * Anthropic client
4. **Storage**

   * SmartBucket for:

     * Per-run artifacts (raw HTML, extracted facts, summaries, metrics)
     * Current browsing/query **policy** as JSON
   * Optional: tiny SQLite/Postgres for quick querying/metrics dashboards (only if you have time).

**Data flow (single run)**

1. Input: `company_name`, optional `domain`, `persona` (e.g., ‚ÄúSDR doing B2B outreach‚Äù).
2. Temporal loads current **BrowsingPolicy** from SmartBuckets.
3. Activity calls Linkup with Anthropic-generated query ‚Üí returns candidate URLs.
4. Activity uses Browser Use to visit URLs and extract structured facts.
5. Activity writes raw pages & extracted facts into SmartBucket.
6. Activity calls Anthropic to:

   * Build **CompanyBrief** (1-pager)
   * Build **OutreachMessage** (LinkedIn/WhatsApp)
   * Produce **page_usefulness_scores**.
7. Activity calls Freepik API to choose a background/graphic and builds a simple ‚Äúaccount card‚Äù (image URL + overlay text).
8. All artifacts + metrics stored as a JSON ‚Äúsnapshot‚Äù in SmartBucket.
9. Workflow returns snapshot ID to the UI.

**Self-learning**

* Separate Temporal workflow (or cron) reads all snapshots + metrics from SmartBucket and:

  * Computes aggregate scores per policy version.
  * Asks Anthropic: ‚ÄúGiven metrics + old policy, write improved BrowsingPolicy + LinkupQueryTemplate.‚Äù
  * Stores new policy (versioned) in SmartBucket.
* Next `ResearchCompanyWorkflow` reads the **latest** policy doc.

---

## 2. Data Models (Pydantic / dataclasses)

Keep models small and JSON-friendly so you can throw them into SmartBuckets easily.

```python
from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict
from datetime import datetime

class CompanyInput(BaseModel):
    name: str
    domain: Optional[str] = None
    persona: str = "SDR researching an account"

class LinkupResult(BaseModel):
    title: str
    url: HttpUrl
    snippet: str
    source: str  # e.g. "website", "linkedin", "news"

class PageExtraction(BaseModel):
    url: HttpUrl
    page_type: str  # "homepage", "pricing", "docs", "linkedin", "other"
    icp: Optional[str] = None
    product_lines: List[str] = []
    pain_points: List[str] = []
    signals: List[str] = []   # e.g. ‚Äúhiring AE in SF‚Äù, ‚ÄúAI features‚Äù
    raw_text_excerpt: str
    usefulness_score: float   # 0‚Äì1 from Claude
    notes: Optional[str] = None

class CompanySnapshot(BaseModel):
    snapshot_id: str
    company: CompanyInput
    created_at: datetime
    policy_version: str
    linkup_results: List[LinkupResult]
    pages: List[PageExtraction]
    brief_md: str             # 1-page markdown
    outreach_message: str     # LinkedIn / WhatsApp text
    freepik_asset_url: Optional[str]

class BrowsingPolicy(BaseModel):
    version: str
    linkup_query_template: str   # f-string template
    max_search_results: int
    allowed_domains: List[str]   # whitelisting by domain suffix
    preferred_paths: List[str]   # e.g. ["/about", "/pricing", "/solutions"]
    max_pages_per_domain: int
    min_usefulness_threshold: float

class RunMetrics(BaseModel):
    snapshot_id: str
    policy_version: str
    company: CompanyInput
    num_linkup_results: int
    num_pages_visited: int
    num_useful_pages: int
    avg_usefulness: float
    tool_failures: Dict[str, int]   # {"linkup": 0, "browser_use": 1, ...}
```

Store `CompanySnapshot` and `RunMetrics` together in SmartBuckets ‚Äì you can either make metrics a subfield on snapshot or a separate object keyed by snapshot ID.

---

## 3. Temporal: Workflows & Activities

### 3.1 Workflows

**Workflow A: `ResearchCompanyWorkflow`**

* **Input:** `CompanyInput`
* **Output:** `snapshot_id: str`

Rough skeleton:

```python
from temporalio import workflow

@workflow.defn
class ResearchCompanyWorkflow:

    @workflow.run
    async def run(self, company: CompanyInput) -> str:
        policy = await workflow.execute_activity(
            load_policy,
            schedule_to_close_timeout=timedelta(seconds=10),
        )

        linkup_results = await workflow.execute_activity(
            fetch_company_data_from_linkup,
            company,
            policy,
            schedule_to_close_timeout=timedelta(seconds=20),
        )

        page_extractions = await workflow.execute_activity(
            browse_and_extract_pages,
            company,
            policy,
            linkup_results,
            schedule_to_close_timeout=timedelta(minutes=5),
        )

        snapshot = await workflow.execute_activity(
            build_snapshot_with_claude,
            company,
            policy,
            linkup_results,
            page_extractions,
            schedule_to_close_timeout=timedelta(seconds=60),
        )

        snapshot_with_visual = await workflow.execute_activity(
            attach_freepik_visual,
            snapshot,
            schedule_to_close_timeout=timedelta(seconds=20),
        )

        snapshot_id = await workflow.execute_activity(
            write_snapshot_to_memory,
            snapshot_with_visual,
            schedule_to_close_timeout=timedelta(seconds=20),
        )

        await workflow.execute_activity(
            log_run_metrics,
            snapshot_with_visual,
            schedule_to_close_timeout=timedelta(seconds=10),
        )

        return snapshot_id
```

**Workflow B: `SelfLearningWorkflow`**

* **Input:** maybe none, just runs on a schedule.
* **Output:** `new_policy_version: str`

High-level:

```python
@workflow.defn
class SelfLearningWorkflow:

    @workflow.run
    async def run(self) -> str:
        snapshots_and_metrics = await workflow.execute_activity(
            fetch_recent_metrics_from_memory,
            schedule_to_close_timeout=timedelta(seconds=30),
        )

        current_policy = await workflow.execute_activity(
            load_policy,
            schedule_to_close_timeout=timedelta(seconds=10),
        )

        new_policy = await workflow.execute_activity(
            propose_new_policy_with_claude,
            current_policy,
            snapshots_and_metrics,
            schedule_to_close_timeout=timedelta(seconds=60),
        )

        new_version = await workflow.execute_activity(
            save_new_policy,
            new_policy,
            schedule_to_close_timeout=timedelta(seconds=10),
        )

        return new_version
```

Trigger this workflow either via **Temporal Schedule** (e.g. hourly) or manually from CLI for the demo.

### 3.2 Activities

Define these as Temporal activities in Python:

1. `load_policy() -> BrowsingPolicy`
2. `fetch_company_data_from_linkup(company, policy) -> list[LinkupResult]`
3. `browse_and_extract_pages(company, policy, linkup_results) -> list[PageExtraction]`
4. `build_snapshot_with_claude(...) -> CompanySnapshot`
5. `attach_freepik_visual(snapshot) -> CompanySnapshot`
6. `write_snapshot_to_memory(snapshot) -> str`
7. `log_run_metrics(snapshot) -> None`
8. `fetch_recent_metrics_from_memory() -> list[RunMetrics]`
9. `propose_new_policy_with_claude(current_policy, metrics) -> BrowsingPolicy`
10. `save_new_policy(policy) -> str`

---

## 4. Integrating Each Tool

### 4.1 Linkup API ‚Äì Research Seed

From docs/marketing, Linkup exposes a simple HTTP API for search, optimized for agents (structured results for URLs, titles, snippets). ([Linkup][1])

**Setup**

* Get API key from their dashboard.
* Store as `LINKUP_API_KEY` in `.env`.

**Activity**: `fetch_company_data_from_linkup`

1. **Generate a query with Claude** based on `company` and `policy.linkup_query_template`:

   ```python
   query_text = policy.linkup_query_template.format(
       company_name=company.name,
       domain=company.domain or "",
       persona=company.persona,
   )
   ```

2. Call Linkup:

   ```python
   async with httpx.AsyncClient(timeout=15.0) as client:
       resp = await client.get(
           "https://api.linkup.so/v1/search",  # check actual path in docs
           params={"q": query_text, "limit": policy.max_search_results},
           headers={"Authorization": f"Bearer {LINKUP_API_KEY}"},
       )
       resp.raise_for_status()
       data = resp.json()
   ```

3. Map results into `LinkupResult` (title, url, snippet, source).

4. Optionally filter by domain (policy.allowed_domains).

5. Return to workflow.

If docs give specialized company endpoints (e.g. ‚Äúenrich company by domain‚Äù), use those instead.

---

### 4.2 Browser Use ‚Äì Deep Page Extraction

Browser Use is an open-source Python library that lets LLMs control the browser (built on Playwright) and extract text / interact with pages. ([GitHub][2])

**Setup**

* Python ‚â• 3.11

* Install:

  ```bash
  uv add browser-use
  uvx browser-use install  # installs Chromium, per docs
  ```

* Configure an LLM. Minimum viable for hackathon:

  * Use whatever Browser Use defaults to (OpenAI) just for navigation.
  * Still use **Anthropic** for extraction & learning.
  * If you have time, implement their ‚Äúcustom LLM‚Äù interface to use Claude for navigation too.

**Activity**: `browse_and_extract_pages`

Strategy:

1. Pre-select URLs from `linkup_results`:

   * Dedicated function `pick_urls_to_visit(linkup_results, policy) -> list[HttpUrl]`:

     * Prefer URLs matching `preferred_paths` or containing ‚Äúabout‚Äù, ‚Äúpricing‚Äù, ‚Äúproduct‚Äù, ‚Äúsolutions‚Äù.

2. For each selected URL:

   * Use Browser Use agent with a **short, focused task**:

     > ‚ÄúGo to {url}. Extract:
     >
     > * ICP (ideal customer profile)
     > * Product lines / key offerings
     > * Common pains they address
     > * Pricing patterns or signals
     >   Return plain text plus your structured JSON fields.‚Äù

3. GET page text / summary back from Browser Use.

   Pseudo-code (API will differ slightly):

   ```python
   from browser_use import Agent, Browser

   async def browse_and_extract_pages(company, policy, linkup_results):
       urls = pick_urls_to_visit(linkup_results, policy)
       browser = await Browser.create()
       agent = Agent(browser=browser)
       extractions = []

       for url in urls:
           task = f"""Visit {url} for {company.name}.
           Extract:
           - ICP
           - Product lines
           - Customer pain points
           Return JSON with fields: page_type, icp, product_lines, pain_points, signals, raw_text_excerpt."""
           agent_result = await agent.run(task=task)
           # Parse agent_result into PageExtraction
           extractions.append(PageExtraction(**parse_json(agent_result)))
       await browser.close()
       return extractions
   ```

4. After each page, call **Anthropic** to:

   * Normalize fields.
   * Produce a `usefulness_score` (0‚Äì1) given your target persona.

   Example prompt:

   > ‚ÄúGiven this extracted info, rate 0‚Äì1 how useful this page is for an SDR trying to understand the company for outbound. Return JSON { "usefulness_score": float }.‚Äù

---

### 4.3 SmartBuckets / SmartMemory ‚Äì Long-Term Memory

SmartBuckets gives you AI-powered storage with automatic indexing, semantic search, and natural language querying. ([Raindrop Developer Hub][3])

**Setup**

* Sign up for LiquidMetal / SmartBuckets (Raindrop).
* Create one SmartBucket, e.g. `self-evolving-agents-bucket`.
* Get API base URL + key: `SMARTBUCKET_API_KEY`.

**Data you store per run**

* Raw artifacts (optional, space-allowing):

  * `company_name/timestamp/homepage.html`
  * `company_name/timestamp/pricing.html`
* Structured snapshot JSON:

  * `company_name/snapshots/{snapshot_id}.json`
* Policy JSON:

  * `config/browsing_policy_v{n}.json`
* (Optional) metrics separate from snapshot:

  * `metrics/{snapshot_id}.json`

**Activity**: `write_snapshot_to_memory`

* Serialize `CompanySnapshot` into JSON bytes.

* `PUT` to SmartBucket upload endpoint:

  ```bash
  curl -X POST https://api.smartbuckets.ai/upload \
    -H "Authorization: Bearer $SMARTBUCKET_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{"bucket": "self-evolving-agents-bucket", "path": "...", "data": {...}}'
  ```

  ([Smart Buckets][5])

* Return `snapshot_id`.

**Activity**: `load_policy`

* If no existing policy, return default hard-coded `BrowsingPolicy`.
* Otherwise:

  * Query SmartBucket for `config/browsing_policy_latest.json` or list objects by prefix `config/`.
  * Download latest version and parse.

**Activity**: `fetch_recent_metrics_from_memory`

* Use SmartBucket‚Äôs query/search API to list last N metrics JSON.
* Example approach:

  * Ask SmartBucket: ‚Äúfind last 100 documents tagged ‚Äòmetrics‚Äô‚Äù (or with path `metrics/`).
  * Parse into `RunMetrics` list.

Because SmartBuckets supports semantic and NL search directly, you can also ask it questions via their query endpoints later if you want ‚Äúshow me all companies where we failed to find pricing info‚Äù. ([Raindrop Developer Hub][3])

---

### 4.4 Freepik API ‚Äì Visual Snapshot

Freepik API lets you programmatically search their library for images/vectors/photos. ([Freepik API][4])

**Setup**

* Sign up, get `FREEPIK_API_KEY`.
* Endpoints usually look like `GET /v1/resources` or similar with `q` and `type` params (check docs).

**Activity**: `attach_freepik_visual`

1. Build a **search query**:

   * Either simple: `"business technology abstract background"`
   * Or from Claude:

     > ‚ÄúGiven this company brief, give me 3 Freepik search tags that best represent the company visually.‚Äù

2. Call Freepik:

   ```python
   async with httpx.AsyncClient(timeout=10) as client:
       resp = await client.get(
           "https://api.freepik.com/v1/resources",
           headers={"X-API-Key": FREEPIK_API_KEY},
           params={"q": search_query, "limit": 1}
       )
       resp.raise_for_status()
       data = resp.json()
       asset_url = data["data"][0]["images"]["preview"]  # adjust to actual shape
   ```

3. Update `snapshot.freepik_asset_url = asset_url`.

In your UI/demo, render this image with overlay text: `company.name`, 3 bullets from `brief_md`, and a ‚Äúrecommended opener‚Äù line from `outreach_message`.

---

### 4.5 Anthropic ‚Äì Reasoning, Extraction, Learning

Use Claude for:

1. **Linkup query generation**
2. **Page extraction normalization + usefulness scoring**
3. **1-page brief & outreach message**
4. **Self-learning policy updates**

Write a helper:

```python
from anthropic import Anthropic

client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

async def claude_json_call(system_prompt: str, user_prompt: str, json_schema: dict):
    resp = client.messages.create(
        model="claude-3-5-sonnet-latest",
        max_tokens=1024,
        system=system_prompt,
        messages=[{"role": "user", "content": user_prompt}],
        extra_body={"response_format": {"type": "json_object", "schema": json_schema}},
    )
    return json.loads(resp.content[0].text)
```

Then:

* For **brief + message**:

  ```python
  result = await claude_json_call(
      system_prompt="You are an expert B2B sales researcher.",
      user_prompt=f"""
      Company: {company.json()}
      Extracted pages: {json.dumps([p.dict() for p in pages])}
      Produce:
      - 1-page markdown brief
      - LinkedIn-style outreach message for {company.persona}.
      """,
      json_schema={...}
  )
  ```

* For **self-learning policy update**:

  ```python
  result = await claude_json_call(
      system_prompt="You optimize strategies for autonomous web research agents.",
      user_prompt=f"""
      CURRENT POLICY:
      {current_policy.json(indent=2)}

      METRICS:
      {json.dumps([m.dict() for m in metrics])}

      TASK:
      - Analyze weaknesses.
      - Propose a minimally changed but improved BrowsingPolicy.
      - Explain briefly why you made each change.
      """,
      json_schema={"type": "object", "properties": {...}}
  )
  ```

You then parse `result["new_policy"]` into `BrowsingPolicy`.

---

## 5. Self-Learning Logic

You want visible but simple adaptation.

### 5.1 Metrics Calculation

In `log_run_metrics(snapshot)`:

* `num_pages_visited = len(snapshot.pages)`
* `num_useful_pages = len([p for p in pages if p.usefulness_score >= 0.6])`
* `avg_usefulness = mean(p.usefulness_score)`
* `num_linkup_results = len(snapshot.linkup_results)`
* `tool_failures` counted in activities (e.g., Linkup timeouts, Browser Use errors, Freepik 4xx).

Store a `RunMetrics` JSON per run.

### 5.2 Learning Objectives for Claude

Give Claude very explicit levers it‚Äôs allowed to touch:

* `linkup_query_template`: change which modifiers you use, e.g., include `"hiring"`, `"pricing"`, `"case studies"`.
* `preferred_paths`: e.g., add `/resources`, `/blog`, `/customers` if those pages often had good info.
* `max_search_results` and `max_pages_per_domain`.
* `min_usefulness_threshold`: threshold under which future runs consider a page as ‚Äúnon-useful‚Äù and may skip similar URLs.

Example ‚Äúlearning task‚Äù:

> ‚ÄúAcross 20 runs, policy v1‚Äôs avg_usefulness is 0.42 and many pages are careers listings. Increase our focus on ‚Äòabout‚Äô, ‚Äòpricing‚Äô, and ‚Äòsolutions‚Äô pages and avoid jobs/careers pages. Update preferred_paths and linkup_query_template accordingly.‚Äù

Claude responds with a new `BrowsingPolicy`. You bump version `v2` and store.

### 5.3 Versioning & Rollout

* Store policy as `config/browsing_policy_v{n}.json`.
* Also keep a stable symlink/object: `config/browsing_policy_latest.json`.
* Workflow always fetches `latest`.
* For demo, you can:

  * Run several companies under `v1`, run `SelfLearningWorkflow` once, then re-run the same company under `v2` and show improved metrics (e.g., more useful pages, more precise brief).

---

## 6. Minimal UI / Demo Flow

Don‚Äôt overbuild. For hackathon, one of these is enough:

### Option A ‚Äì Simple Web UI (FastAPI + HTMX or Streamlit)

* Input form: `company_name`, `domain` (optional), `persona`.

* Button **‚ÄúRun Research Agent‚Äù**:

  * Calls your backend, which:

    * Starts `ResearchCompanyWorkflow`.
    * Polls Temporal for completion (or uses async + WebSocket if you‚Äôre feeling fancy).

* Result view:

  * Show `brief_md` rendered.
  * Show `outreach_message`.
  * Show Freepik image and overlay top facts (company name, 3 key bullets).
  * Show metrics: `num_pages_visited`, `avg_usefulness`, `policy_version`.

* For **self-learning demo**:

  * Pre-run `SelfLearningWorkflow`.
  * Show ‚ÄúPolicy v1 vs v2 diff‚Äù (from Claude‚Äôs explanation string).

### Option B ‚Äì CLI + Static HTML

* CLI command: `python run_company.py "Acme Corp" --domain acme.com`.
* It prints progress and writes a `snapshot.html` file with:

  * Markdown ‚Üí HTML convert.
  * `<img src="{freepik_asset_url}">`.
* For presentation, pre-open a nice snapshot in the browser.

---

## 7. Day-Of Hackathon Execution Plan

You‚Äôve got ~5‚Äì6 real hours of coding. I‚Äôd time-box like this:

**Hour 1 ‚Äì Skeleton & Infra**

* Set up repo structure:

  * `workflows/`, `activities/`, `clients/`, `models.py`, `app.py`.
* Install deps: Temporal SDK, httpx, pydantic, anthropic, browser-use.
* Wire Anthropic client + `.env`.
* Hard-code dummy data and get a trivial Temporal workflow running.

**Hour 2 ‚Äì Linkup + Browser Use Path**

* Implement `CompanyInput`, `LinkupResult`, `PageExtraction`.
* Implement `fetch_company_data_from_linkup` with real API call.
* Implement `browse_and_extract_pages` with Browser Use (even if you stub actual extraction and just return `PageExtraction` with `icp=None` initially).
* Make sure you can see a list of URLs and some extracted text.

**Hour 3 ‚Äì Snapshot + SmartBuckets**

* Implement `CompanySnapshot`, `RunMetrics`.
* Implement `build_snapshot_with_claude`:

  * Summaries + outreach message.
  * Usefulness scores per page.
* Implement `write_snapshot_to_memory` with SmartBucket.
* Confirm you can see your JSON object in SmartBucket dashboard.

**Hour 4 ‚Äì Freepik + Basic UI**

* Implement `attach_freepik_visual`.
* Build a minimal web UI or CLI that:

  * Triggers the workflow.
  * Shows brief + message + visual.

**Hour 5 ‚Äì Self-Learning MVP**

* Implement `log_run_metrics`.
* Manually run 5‚Äì10 companies ‚Üí accumulate metrics.
* Implement `SelfLearningWorkflow` and `propose_new_policy_with_claude`.
* Show at least one policy update (even if changes are modest).

**Final polish (whatever time remains)**

* Tighten prompts for more deterministic outputs.
* Add clear sponsor mentions in README + demo narrative:

  * ‚ÄúWe use Linkup for real-time web/company search‚Ä¶‚Äù
  * ‚ÄúSmartBuckets as the agent‚Äôs long-term memory‚Ä¶‚Äù
  * ‚ÄúFreepik to instantly generate visual account snapshots‚Ä¶‚Äù
* Prepare a 3-minute talk track + live demo flow.

---

If you want, next step we can:

* Design **exact Claude prompts** for each step (Linkup query, page extraction, brief, policy learning), or
* Sketch the **folder structure + `pyproject.toml`** so you can bootstrap this in Cursor and start filling activities.

[1]: https://www.linkup.so/?utm_source=chatgpt.com "Linkup - World's best search for AI Apps"
[2]: https://github.com/browser-use/browser-use?utm_source=chatgpt.com "browser-use/browser-use: üåê Make websites accessible for ..."
[3]: https://docs.liquidmetal.ai/concepts/smartbuckets/?utm_source=chatgpt.com "SmartBuckets"
[4]: https://docs.freepik.com/?utm_source=chatgpt.com "Welcome to Freepik API - Freepik API"
[5]: https://smartbuckets.ai/?utm_source=chatgpt.com "Smart Buckets - AI-Driven Storage for Every Developer"
